{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing done\n",
    "\n",
    "Data:\n",
    "- Order: calculate supply and demand\n",
    "- Weather: PM 2.5, weather etc.\n",
    "- POI: point of interest, refers to facilities such as resturant, theatres etc.\n",
    "- Traffic: four levels of traffic jam.\n",
    "- cluster: Map district hash to district id.\n",
    "\n",
    "All data are precocessed at this point.\n",
    "\n",
    "intuitively, weather, taffic, POI should all affect didi's request volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['time','weather','temperature','PM25','district_ID','date','week','req','ans','gap']\n",
    "df = pd.read_csv('/resources/data/DIDI/summaries/output.csv', \n",
    "                 names = cols, )\n",
    "\n",
    "droping = ['req','ans', 'date']\n",
    "df.drop(droping, axis=1, inplace=True)\n",
    "\n",
    "df = df.rename(columns={'gap' : 'y'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>PM25</th>\n",
       "      <th>district_ID</th>\n",
       "      <th>week</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  weather  temperature  PM25  district_ID  week    y\n",
       "0     1        1          4.0   177            1     4  9.0\n",
       "1     1        1          4.0   177            2     4  1.0\n",
       "2     1        1          4.0   177            3     4  1.0\n",
       "3     1        1          4.0   177            4     4  3.0\n",
       "4     1        1          4.0   177            5     4  0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utilitiy functions\n",
    "\n",
    "def onehot_encode(df, cols):\n",
    "    return pd.get_dummies(df, columns=cols)\n",
    "\n",
    "def normalize(df, df_ref, columns):\n",
    "    for col in columns:\n",
    "        df[col] = (df[col] - df_ref[col].mean()) / df_ref[col].std()\n",
    "    return df\n",
    "\n",
    "def MAPE(D, pred, act):\n",
    "    D['pred'] = pred\n",
    "    D['act'] = act\n",
    "    D['delta'] = [np.abs((D.iloc[i]['act'] - D.iloc[i]['pred'])/D.iloc[i]['act']) if D.iloc[i]['act']> 0 else 0\n",
    "                  for i in xrange(D.shape[0])]\n",
    "    subMAPE = D['delta'].mean()\n",
    "    return subMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>PM25</th>\n",
       "      <th>district_ID</th>\n",
       "      <th>week</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  weather  temperature  PM25  district_ID  week    y\n",
       "0     1        1          4.0   177            1     4  9.0\n",
       "1     1        1          4.0   177            2     4  1.0\n",
       "2     1        1          4.0   177            3     4  1.0\n",
       "3     1        1          4.0   177            4     4  3.0\n",
       "4     1        1          4.0   177            5     4  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_col = ['PM25', 'temperature']\n",
    "normalizers = set(df[normal_col])\n",
    "\n",
    "#normalize none categorical columns.\n",
    "#df = normalize(df,df,normalizers)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>PM25</th>\n",
       "      <th>district_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>week_0</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>week_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  weather  temperature  PM25  district_ID    y  week_0  week_1  week_2  \\\n",
       "0     1        1          4.0   177            1  9.0     0.0     0.0     0.0   \n",
       "1     1        1          4.0   177            2  1.0     0.0     0.0     0.0   \n",
       "2     1        1          4.0   177            3  1.0     0.0     0.0     0.0   \n",
       "3     1        1          4.0   177            4  3.0     0.0     0.0     0.0   \n",
       "4     1        1          4.0   177            5  0.0     0.0     0.0     0.0   \n",
       "\n",
       "   week_3  week_4  week_5  week_6  \n",
       "0     0.0     1.0     0.0     0.0  \n",
       "1     0.0     1.0     0.0     0.0  \n",
       "2     0.0     1.0     0.0     0.0  \n",
       "3     0.0     1.0     0.0     0.0  \n",
       "4     0.0     1.0     0.0     0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot ecoding catoegorical columns.\n",
    "df = onehot_encode(df, ['week'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cols = df.co['time','weather','temperature','PM25','districtID', 'week']\n",
    "cols = (list(set(df.columns).difference(['y'])))\n",
    "index = np.array(df[['district_ID', 'time']])\n",
    "\n",
    "X = np.array(df[cols])\n",
    "y = np.array(df['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Training, Validation and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "$$ MAPE = \\frac 1 D \\sum_{d=1}^D \\frac 1 T \\sum_{t=1}^T abs \\left( \\frac {\\hat y_{dt} - y_{dt}} {y_{dt}} \\right) \\forall y_{dt} > 0$$ where $D$ is the total number of districts, $T$ is the total number of time intervals for a given district, $\\hat y_{dt}$ is our estimate of the gap and $y_{dt}$ the actual gap for district $d$ at time $t$. Notice that we exclude all data points with zero actual gap from the calculation of MAPE, and that we take the sum over the time intervals within a district before we sum over districts. \n",
    "\n",
    "Mape is non convex and hard to differentiate, approximate it with abs distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317526, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317526,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280000, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:280000,]\n",
    "y_train = y[:280000,]\n",
    "index_train = index[:280000,]\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37526, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = X[280000:317526,]\n",
    "y_val = y[280000:317526,]\n",
    "index_val = index[280000:317526,]\n",
    "index_val = pd.DataFrame(index_val, columns=['district_ID', 'UTC'])\n",
    "\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>PM25</th>\n",
       "      <th>district_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>week_0</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>week_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  weather  temperature  PM25  district_ID    y  week_0  week_1  week_2  \\\n",
       "0     1        1          4.0   177            1  9.0     0.0     0.0     0.0   \n",
       "1     1        1          4.0   177            2  1.0     0.0     0.0     0.0   \n",
       "2     1        1          4.0   177            3  1.0     0.0     0.0     0.0   \n",
       "3     1        1          4.0   177            4  3.0     0.0     0.0     0.0   \n",
       "4     1        1          4.0   177            5  0.0     0.0     0.0     0.0   \n",
       "\n",
       "   week_3  week_4  week_5  week_6  \n",
       "0     0.0     1.0     0.0     0.0  \n",
       "1     0.0     1.0     0.0     0.0  \n",
       "2     0.0     1.0     0.0     0.0  \n",
       "3     0.0     1.0     0.0     0.0  \n",
       "4     0.0     1.0     0.0     0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just guessing 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23467365435568543"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = y_val.shape[0]\n",
    "MAPE(index_val, np.ones(l), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-layer network, 200 neurons.\n",
    "\n",
    "## Activation function candidates:\n",
    "- Linear\n",
    "- RELU \n",
    "- ELU\n",
    "- Sigmoid\n",
    "- Softmax <<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.422364\n",
      "Validation MAPE: 0.48064\n",
      "Minibatch loss at step 500: 0.216691\n",
      "Validation MAPE: 0.23651\n",
      "Minibatch loss at step 1000: 0.202043\n",
      "Validation MAPE: 0.23589\n",
      "Minibatch loss at step 1500: 0.271510\n",
      "Validation MAPE: 0.23534\n",
      "Minibatch loss at step 2000: 0.266236\n",
      "Validation MAPE: 0.23542\n",
      "Minibatch loss at step 2500: 0.222944\n",
      "Validation MAPE: 0.23503\n",
      "Minibatch loss at step 3000: 0.225983\n",
      "Validation MAPE: 0.23486\n",
      "Minibatch loss at step 3500: 0.301049\n",
      "Validation MAPE: 0.23482\n",
      "Minibatch loss at step 4000: 0.257672\n",
      "Validation MAPE: 0.23473\n",
      "Minibatch loss at step 4500: 0.248577\n",
      "Validation MAPE: 0.23474\n",
      "Minibatch loss at step 5000: 0.221139\n",
      "Validation MAPE: 0.23475\n",
      "Minibatch loss at step 5500: 0.253877\n",
      "Validation MAPE: 0.23472\n",
      "Minibatch loss at step 6000: 0.254884\n",
      "Validation MAPE: 0.23469\n",
      "Minibatch loss at step 6500: 0.257774\n",
      "Validation MAPE: 0.23472\n",
      "Minibatch loss at step 7000: 0.282337\n",
      "Validation MAPE: 0.23469\n",
      "Minibatch loss at step 7500: 0.270973\n",
      "Validation MAPE: 0.23469\n",
      "Minibatch loss at step 8000: 0.231589\n",
      "Validation MAPE: 0.23468\n",
      "Minibatch loss at step 8500: 0.207331\n",
      "Validation MAPE: 0.23468\n",
      "Minibatch loss at step 9000: 0.199490\n",
      "Validation MAPE: 0.23468\n",
      "Minibatch loss at step 9500: 0.284456\n",
      "Validation MAPE: 0.23468\n",
      "Minibatch loss at step 10000: 0.194646\n",
      "Validation MAPE: 0.23468\n",
      "Minibatch loss at step 10500: 0.289413\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 11000: 0.225384\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 11500: 0.228893\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 12000: 0.227434\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 12500: 0.186578\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 13000: 0.310849\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 13500: 0.258803\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 14000: 0.248368\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 14500: 0.278237\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 15000: 0.230697\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 15500: 0.225078\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 16000: 0.270190\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 16500: 0.216130\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 17000: 0.226234\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 17500: 0.245201\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 18000: 0.197389\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 18500: 0.232518\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 19000: 0.243944\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 19500: 0.274698\n",
      "Validation MAPE: 0.23467\n",
      "Minibatch loss at step 20000: 0.275514\n",
      "Validation MAPE: 0.23467\n",
      "This took 878.14 seconds\n"
     ]
    }
   ],
   "source": [
    "def I(X):\n",
    "    return X\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "num_labels = 1\n",
    "pop_size = y_train.shape[0]\n",
    "features = X_train.shape[1]\n",
    "batch_size = 128\n",
    "\n",
    "num_hidden_nodes = 200\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, features))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, ))\n",
    "    Xv = tf.constant(X_val, tf.float32)\n",
    "    #Xt = tf.constant(X_test, tf.float32)\n",
    "    \n",
    "    '''\n",
    "    Choose the flavor of your neuron\n",
    "        set F = flavor, where flavor is the neuron flavor. Choose the flavor from the following list\n",
    "            linear: I\n",
    "            RELU: tf.nn.relu\n",
    "            ELU: tf.nn.elu\n",
    "            sigmoid: tf.nn.sigmoid\n",
    "            softmax: tf.nn.softmax\n",
    "    '''\n",
    "    F = tf.nn.softmax\n",
    "        \n",
    "    with tf.name_scope('hidden'):\n",
    "        fros = features\n",
    "        tos = num_hidden_nodes\n",
    "        weights = tf.Variable(tf.truncated_normal([fros, tos], stddev= 1/ tf.sqrt(float(fros)),  \n",
    "                                                  name='weights'))\n",
    "        biases = tf.Variable(tf.zeros([tos]),  name='biases')\n",
    "        H = F(tf.matmul(X, weights) + biases)\n",
    "        Hv = F(tf.matmul(Xv, weights) + biases)\n",
    "        #Ht = F(tf.matmul(Xt, weights) + biases)\n",
    "    \n",
    "    with tf.name_scope('output'):\n",
    "        F = tf.nn.elu\n",
    "        fros = num_hidden_nodes\n",
    "        tos = num_labels\n",
    "        weights = tf.Variable(tf.truncated_normal([fros, tos], stddev= 1/ tf.sqrt(float(fros)),  \n",
    "                                                  name='weights'))\n",
    "        biases = tf.Variable(tf.zeros([tos]),  name='biases')\n",
    "        yfit = F(tf.matmul(H, weights) + biases)\n",
    "        yval = F(tf.matmul(Hv, weights) + biases)\n",
    "        #ytest = F(tf.matmul(Ht, weights) + biases)        \n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.5)\n",
    "    loss = tf.reduce_mean(tf.abs((y - yfit)/(y + 1./y)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "import time\n",
    "num_steps = 20001\n",
    "start = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in xrange(num_steps):\n",
    "        # Generate a minibatch.\n",
    "        indices = np.random.choice(pop_size, batch_size, replace=False)\n",
    "        batch_data = X_train[indices, :]\n",
    "        batch_labels = y_train[indices]\n",
    "        # prep and feed\n",
    "        feed_dict = {X : batch_data, y : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, yfit], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Validation MAPE: %.5f\" % MAPE((index_val.copy()), yval.eval(), y_val))\n",
    "    #print np.mean(ytest.eval())\n",
    "    #print np.std(ytest.eval())\n",
    "\n",
    "print ('This took %.2f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network, 200 neurons each.  \n",
    "\n",
    "## activation function candidates:\n",
    "\n",
    "- Linear\n",
    "- RELU <<<<\n",
    "- ELU\n",
    "- Sigmoid\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.345919\n",
      "Validation MAPE: 0.46793\n",
      "Minibatch loss at step 500: 0.197590\n",
      "Validation MAPE: 0.23552\n",
      "Minibatch loss at step 1000: 0.274812\n",
      "Validation MAPE: 0.23672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-46696371dd7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# Generate a minibatch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def I(X):\n",
    "    return X\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "num_labels = 1\n",
    "pop_size = y_train.shape[0]\n",
    "features = X_train.shape[1]\n",
    "batch_size = 128\n",
    "\n",
    "num_hidden_nodes = 200\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, features))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, ))\n",
    "    Xv = tf.constant(X_val, tf.float32)\n",
    "    #Xt = tf.constant(X_test, tf.float32)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Choose the flavor of your neuron in the first line of each name scope below\n",
    "        set F = flavor, where flavor is the neuron flavor. Choose the flavor from the following list\n",
    "            linear: I\n",
    "            RELU: tf.nn.relu\n",
    "            ELU: tf.nn.elu\n",
    "            sigmoid: tf.nn.sigmoid\n",
    "            softmax: tf.nn.softmax\n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        F = tf.nn.softmax\n",
    "        fros = features\n",
    "        tos = num_hidden_nodes\n",
    "        weights = tf.Variable(tf.truncated_normal([fros, tos], stddev= 1/ tf.sqrt(float(fros)),  \n",
    "                                                  name='weights'))\n",
    "        biases = tf.Variable(tf.zeros([tos]),  name='biases')\n",
    "        H = F(tf.matmul(X, weights) + biases)\n",
    "        Hv = F(tf.matmul(Xv, weights) + biases)\n",
    "        #Ht = F(tf.matmul(Xt, weights) + biases)\n",
    "\n",
    "        \n",
    "    with tf.name_scope('hidden2'):\n",
    "        F = tf.nn.relu\n",
    "        fros = num_hidden_nodes\n",
    "        tos = num_hidden_nodes\n",
    "        weights = tf.Variable(tf.truncated_normal([fros, tos], stddev= 1/ tf.sqrt(float(fros)),  \n",
    "                                                  name='weights'))\n",
    "        biases = tf.Variable(tf.zeros([tos]),  name='biases')\n",
    "        H = F(tf.matmul(H, weights) + biases)\n",
    "        Hv = F(tf.matmul(Hv, weights) + biases)\n",
    "        #Ht = F(tf.matmul(Ht, weights) + biases)\n",
    "\n",
    "        \n",
    "    with tf.name_scope('output'):\n",
    "        F = tf.nn.elu\n",
    "        fros = num_hidden_nodes\n",
    "        tos = num_labels\n",
    "        weights = tf.Variable(tf.truncated_normal([fros, tos], stddev= 1/ tf.sqrt(float(fros)),  \n",
    "                                                  name='weights'))\n",
    "        biases = tf.Variable(tf.zeros([tos]),  name='biases')\n",
    "        yfit = F(tf.matmul(H, weights) + biases)\n",
    "        yval = F(tf.matmul(Hv, weights) + biases)\n",
    "        #ytest = F(tf.matmul(Ht, weights) + biases)        \n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.5)\n",
    "    loss = tf.reduce_mean(tf.abs((y - yfit)/(y + 1./y)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "import time\n",
    "num_steps = 20001\n",
    "start = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in xrange(num_steps):\n",
    "        # Generate a minibatch.\n",
    "        indices = np.random.choice(pop_size, batch_size, replace=False)\n",
    "        batch_data = X_train[indices, :]\n",
    "        batch_labels = y_train[indices]\n",
    "        # prep and feed\n",
    "        feed_dict = {X : batch_data, y : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, yfit], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Validation MAPE: %.5f\" % MAPE((index_val.copy()), yval.eval(), y_val))\n",
    "    #print np.mean(ytest.eval())\n",
    "    #print np.std(ytest.eval())\n",
    "\n",
    "print ('This took %.2f seconds' % (time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
